{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu69LyL_RLe-",
        "outputId": "0876a617-d1a9-427e-a5a0-8dc7f3ca66d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Setup\n",
        "#!pip install -q pandas\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "87Rdq-97jafY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Load CSV\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Annam.ai\"\n",
        "\n",
        "# Automatically detect your _cleaned_dataset.csv file\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\"_dataset_cleaned.csv\")]\n",
        "if not csv_files:\n",
        "    raise Exception(\"No *_cleaned_dataset.csv file found in Annam.ai folder!\")\n",
        "\n",
        "file_name = csv_files[0]\n",
        "file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\"âœ… Loaded: {file_name} with {len(df)} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aafqjGkFRwi6",
        "outputId": "805c7ac5-b3e0-4249-f36e-647192d7952f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded: ARUNACHAL PRADESH_dataset_cleaned.csv with 2039 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load bad word list\n",
        "# Create it if not present\n",
        "bad_words_path = os.path.join(folder_path, \"bad_words.txt\")\n",
        "\n",
        "# Create file if not exists\n",
        "if not os.path.exists(bad_words_path):\n",
        "    with open(bad_words_path, \"w\") as f:\n",
        "        pass\n",
        "\n",
        "# Load current list\n",
        "with open(bad_words_path, \"r\") as f:\n",
        "    unwanted_words = [line.strip().lower() for line in f.readlines() if line.strip()]\n",
        "\n",
        "print(f\"ðŸ›‘ {len(unwanted_words)} unwanted words loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKIWIB7wRzZo",
        "outputId": "121fa9e3-51e0-428d-d11d-9280e1164294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ›‘ 148 unwanted words loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Drop rows with NaN or completely empty or whitespace-only QueryText\n",
        "df = df[df['QueryText'].notna()]\n",
        "df = df[df['QueryText'].str.strip() != \"\"]\n",
        "\n",
        "# Remove rows where QueryText has no alphabets (i.e., only numbers or symbols)\n",
        "df = df[df['QueryText'].apply(lambda x: bool(re.search(r'[a-zA-Z]', str(x))))]\n",
        "\n",
        "print(f\"âœ… Cleaned dataframe shape: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW0DuJFFjU1I",
        "outputId": "7812eb11-4821-4157-91d9-29fa7acdcd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaned dataframe shape: (2039, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where QueryText has only one word AND length â‰¤ 2\n",
        "df = df[~df['QueryText'].apply(lambda x: len(x.strip().split()) == 1 and len(x.strip()) <= 2)]\n",
        "\n",
        "print(f\"âœ… Data after removing 1-word queries with â‰¤2 letters: {df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJcEh7PGqIa5",
        "outputId": "db317958-fdad-4153-cd33-90767463a32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data after removing 1-word queries with â‰¤2 letters: (2039, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def looks_like_gibberish(text):\n",
        "    if pd.isna(text):\n",
        "        return False\n",
        "\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # Rule 0: Remove non-alpha characters for basic analysis\n",
        "    cleaned_text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Rule A: Only for single word or very short phrases\n",
        "    if len(cleaned_text.split()) > 2:\n",
        "        return False\n",
        "\n",
        "    # Rule B: Too short or too long\n",
        "    if len(cleaned_text.replace(\" \", \"\")) <= 2 or len(cleaned_text.replace(\" \", \"\")) > 15:\n",
        "        return True\n",
        "\n",
        "    # Rule C: No vowels at all\n",
        "    if not re.search(r'[aeiou]', cleaned_text):\n",
        "        return True\n",
        "\n",
        "    # Rule D: Repeating character patterns (like jhjhjhjh, abababa)\n",
        "    if re.match(r'^(.{1,3})\\1{2,}$', cleaned_text.replace(\" \", \"\")):\n",
        "        return True\n",
        "\n",
        "    # Rule E: Mostly consonants (>80%)\n",
        "    letters_only = re.sub(r'\\s+', '', cleaned_text)\n",
        "    consonants = re.findall(r'[bcdfghjklmnpqrstvwxyz]', letters_only)\n",
        "    if len(letters_only) > 0 and len(consonants) / len(letters_only) > 0.8:\n",
        "        return True\n",
        "\n",
        "    # Rule F: Detect weird structure like \"cg gfghcf\"\n",
        "    for word in cleaned_text.split():\n",
        "        if len(word) > 3 and re.search(r'[^aeiou]{4,}', word):  # 4+ consonants together\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Apply filter\n",
        "df = df[~df['QueryText'].astype(str).apply(looks_like_gibberish)]\n",
        "\n",
        "print(f\"âœ… Removed gibberish-looking one-word queries. Rows remaining: {len(df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6HW6debx0QL",
        "outputId": "141b5091-6b99-4510-8831-59d91859e830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Removed gibberish-looking one-word queries. Rows remaining: 2015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Drop rows with only one word in QueryText\n",
        "# df = df[df['QueryText'].astype(str).str.strip().str.split().apply(len) > 1]\n",
        "\n",
        "# print(f\"âœ… Rows with only one word removed. Remaining rows: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "Hg2o59TIq8rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5cZkeqxn3bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Remove rows with <=2 words and containing unwanted words\n",
        "def is_bad_query(text):\n",
        "    if pd.isna(text): return False\n",
        "    words = text.lower().split()\n",
        "    if len(words) > 2: return False\n",
        "    return any(word in words for word in unwanted_words)\n",
        "\n",
        "before_count = len(df)\n",
        "df_cleaned = df[~df['QueryText'].apply(is_bad_query)]\n",
        "removed = before_count - len(df_cleaned)\n",
        "\n",
        "print(f\"ðŸ§¹ Removed {removed} bad rows. Remaining rows: {len(df_cleaned)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdn8RsDwR1qP",
        "outputId": "e8441910-c846-4f60-9f10-92bcd23fa473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Removed 229 bad rows. Remaining rows: 1786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zt3cpLEmjOVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Save cleaned file\n",
        "cleaned_filename = file_name.replace(\"_dataset_cleaned.csv\", \"_final_dataset.csv\")\n",
        "cleaned_path = os.path.join(folder_path, cleaned_filename)\n",
        "df_cleaned.to_csv(cleaned_path, index=False)\n",
        "\n",
        "print(f\"âœ… Cleaned data saved to: {cleaned_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52p-n0dbR4O4",
        "outputId": "e07a73c8-1f92-4155-bbc6-fba24142effe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaned data saved to: /content/drive/MyDrive/Annam.ai/ARUNACHAL PRADESH_final_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Generate remaining short phrases for review\n",
        "remaining_short = (\n",
        "    df_cleaned[df_cleaned['QueryText'].notna()]  # Remove NaNs\n",
        "    .assign(QueryText=df_cleaned['QueryText'].astype(str))  # Force string type\n",
        "    [lambda x: x['QueryText'].str.split().apply(len) <= 2]['QueryText']\n",
        "    .unique()\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "\n",
        "review_path = os.path.join(folder_path, \"next_review_short_phrases.csv\")\n",
        "pd.DataFrame(remaining_short, columns=[\"QueryText\"]).to_csv(review_path, index=False)\n",
        "\n",
        "print(f\"âœ… Saved remaining short queries to: {review_path}\")\n",
        "\n",
        "\n",
        "print(f\"ðŸ§ Next review file saved to: {review_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy6R6NuHR554",
        "outputId": "91ce9335-73d8-4ee6-8e14-734844f045ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved remaining short queries to: /content/drive/MyDrive/Annam.ai/next_review_short_phrases.csv\n",
            "ðŸ§ Next review file saved to: /content/drive/MyDrive/Annam.ai/next_review_short_phrases.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and edit manually in Colab\n",
        "bad_words_path = \"/content/drive/MyDrive/Annam.ai/bad_words.txt\"\n",
        "\n",
        "# View current bad words\n",
        "with open(bad_words_path, 'r') as f:\n",
        "    bad_words = [line.strip() for line in f]\n",
        "\n",
        "print(\"âœï¸ Current bad words:\")\n",
        "print(bad_words)\n",
        "\n",
        "# Manually add new ones (example)\n",
        "bad_words.extend([\"irrelevant\", \"vety\"])\n",
        "\n",
        "# Save updated list\n",
        "with open(bad_words_path, 'w') as f:\n",
        "    f.write('\\n'.join(set(bad_words)))  # remove duplicates\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmh6p_IYXo65",
        "outputId": "b8270160-a8ad-4c79-b1cc-d95296772ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœï¸ Current bad words:\n",
            "['WEATHHER', 'vety q', 'Sound', 'disconected', 'Details', 'ACD', 'farmer', 'WEATER', 'test  cal', 'Test-Call', 'report', 'WEATHE', 'cal transpered', 'vbbs', 'pm kisan', 'diss', 'dis continive', 'update', 'DIS', 'details', 'RAQINFALL', 'IN4MATION', 'cond', 'redail', 'Disconnect', 'test cll', 'rain fall', 'INCOMPLETE', 'manipuri call', 'WEATEHER', 'weatherin', 'pm-kisan e-kyc', 'registration', 'monsoon', 'Busy', 'call', 'TestCall', 'tell', 'VET', 'wether', 'manipuri farmer', 'WEATHEER', 'manipuri q', 'weatehr', 'SAME', 'Internet', 'time pas', 'rainfal', 'same', 'hello', 'weathere', 'cal', 'not', 'farmers registration', 'remote', 'CCALL', 'weather', 'network', 'climate', 'MANSOON', 'FOECAST', 'test', 'NUMBER', 'test  call', 'tasting', 'FORECAST', 'REPORT', 'forecast', 'manipuri language', 'id', 'price', 'rain', 'WEARHER', 'How', 'monsoons', 'FORCAST', 'WAETHER', 'irrelivant cal', 'diissconected', 'tesy call', 'Thank You', 'ABOUT', 'conmd', 'vety quary', 'blank call', 'climete', 'waatherof', 'number', 'VOICE', 'ThankYou', 'BUSY', 'WEAHER', 'blank', 'ran', 'condition', 'wheather', 'Apply', 'office', 'wrong', 'OTHERS', 'manipur q', 'infomation', 'CALLD, DIS, REDAIL, SURE, redail, CCALL', 'VETYCALL', 'call disconnected', 'mobile', 'rates', '232523772354 disconnect2404', 'busy', 'OTHER', 'Disconnected', 'monsan', 'acd', 'testing', 'rate', 'monsun', 'rainfall', 'WEAATHER', 'test calls', 'no', 'QUERY ESCALATE', 'disconect', 'HINDI', 'call---09', 'rep0rt', 'undefined', 'bhav', 'contect', 'agri qwerry', 'VETCALL', 'REDAIL', 'SURE', 'how', 'UPDATE', 'info', 'information', 'WAATHER', 'codition', 'climite', 'DISCOMMECTED', 'CALLD', 'MARKET', 'test call', 'INFORM', 'CONTACT', 'INFOR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "67HWnt3gr_sM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}